{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm2rKI1xd0Tu+mpVKm4fFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthCoder501/GAAP/blob/main/OneDrive_Dataset_Download.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd0tQ4Q_lmGr"
      },
      "outputs": [],
      "source": [
        "# Install rclone to enable syncing and mounting of OneDrive\n",
        "!curl https://rclone.org/install.sh | sudo bash"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch rclone configuration interface\n",
        "!rclone config"
      ],
      "metadata": {
        "id": "ZybR3veRlv8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update packages and install FUSE3\n",
        "!apt-get update && apt-get install -y fuse3"
      ],
      "metadata": {
        "id": "ldmEBGtglv5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a local directory where OneDrive will be mounted\n",
        "!mkdir -p /content/MyOneDrive"
      ],
      "metadata": {
        "id": "KsEsQ59Llv3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the remote OneDrive directory to the local path using rclone\n",
        "!rclone mount MyOneDrive: /content/MyOneDrive --vfs-cache-mode full --allow-other --daemon"
      ],
      "metadata": {
        "id": "29XZGEmmlv0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-storage-blob"
      ],
      "metadata": {
        "id": "GlH5VFVEnKPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from azure.storage.blob import BlobServiceClient, ContainerClient\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests"
      ],
      "metadata": {
        "id": "F62MhK73lvyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sas_url = \"https://aimistanforddatasets01.blob.core.windows.net/inspect2?sv=2019-02-02&sr=c&sig=vFSGSEsx2MsE4AY3rIEVO%2F0ijrEh7FjwYON1FUU6fUU%3D&st=2025-06-27T18%3A29%3A09Z&se=2025-07-27T18%3A34%3A09Z&sp=rl\"\n",
        "csv_path = \"/content/Progression Dataset.csv\"\n",
        "download_dir = \"/content/MyOneDrive/GAAP Research Resources/Imaging & Impressions Dataset\"  # Changed to OneDrive path\n",
        "os.makedirs(download_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "a-RY7WFtlvvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV with desired impression_ids\n",
        "df = pd.read_csv(csv_path)\n",
        "valid_ids = set(df['impression_id'].astype(str))  # ensure all are strings\n",
        "expected_files = {f\"{impression_id}.nii.gz\" for impression_id in valid_ids}\n",
        "\n",
        "# Connect to blob container using SAS URL\n",
        "container_client = ContainerClient.from_container_url(container_url=sas_url)"
      ],
      "metadata": {
        "id": "LkloZ3xQlvsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify inputs\n",
        "print(f\"CSV Path: {csv_path}\")\n",
        "print(f\"Found {len(valid_ids)} impression IDs in CSV\")\n",
        "print(\"First 5 IDs:\", list(valid_ids)[:5])\n",
        "print(\"------------------------------------------------\")\n",
        "sample_blobs = [blob.name for i, blob in enumerate(container_client.list_blobs()) if i < 5]\n",
        "print(\"Blob Names\")\n",
        "print(sample_blobs)"
      ],
      "metadata": {
        "id": "bAcZ7wlklvqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Matching logic'''\n",
        "\n",
        "print(f\"Found {len(valid_ids)} impression IDs in CSV\")\n",
        "print(\"First 5 IDs:\", list(valid_ids)[:5])\n",
        "\n",
        "# Update expected_files to include the CTPA/ prefix\n",
        "expected_files = {f\"CTPA/{imp_id}.nii.gz\" for imp_id in valid_ids}\n",
        "\n",
        "# Tracking for future potential debug\n",
        "downloaded_count = 0\n",
        "skipped_count = 0"
      ],
      "metadata": {
        "id": "pLQy7fQhGG5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of files already downloaded\n",
        "existing_files = {f for f in os.listdir(download_dir) if f.endswith('.nii.gz')}"
      ],
      "metadata": {
        "id": "wGILYep-HXeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(existing_files))"
      ],
      "metadata": {
        "id": "DXXqsgSRHZxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download files\n",
        "downloaded_count = 0\n",
        "for blob in container_client.list_blobs():\n",
        "    if blob.name in expected_files:\n",
        "        # Extract just the filename part (removes CTPA/ prefix)\n",
        "        filename = blob.name.split('/')[-1]\n",
        "        dest_path = os.path.join(download_dir, filename)\n",
        "\n",
        "        print(f\"Downloading: {blob.name} -> {dest_path}\")\n",
        "        blob_client = container_client.get_blob_client(blob)\n",
        "\n",
        "        with open(dest_path, \"wb\") as f:\n",
        "            f.write(blob_client.download_blob().readall())\n",
        "        downloaded_count += 1\n",
        "\n",
        "print(f\"\\nDownload complete! {downloaded_count} files saved to {download_dir}\")\n",
        "\n",
        "# Verify\n",
        "if downloaded_count > 0:\n",
        "    print(\"\\nFirst 5 downloaded files:\")\n",
        "    !ls -lh \"{download_dir}\" | head -5"
      ],
      "metadata": {
        "id": "LuAlTQd3rmwQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To download additional files"
      ],
      "metadata": {
        "id": "sa0HNFz9alh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for blob in container_client.list_blobs():\n",
        "    if blob.name in expected_files:\n",
        "        # Extract just the filename part (removes CTPA/ prefix)\n",
        "        filename = blob.name.split('/')[-1]\n",
        "        dest_path = os.path.join(download_dir, filename)\n",
        "\n",
        "        # Skip if file already exists\n",
        "        if filename in existing_files:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        print(f\"Downloading: {blob.name} -> {dest_path}\")\n",
        "        blob_client = container_client.get_blob_client(blob)\n",
        "\n",
        "        with open(dest_path, \"wb\") as f:\n",
        "            f.write(blob_client.download_blob().readall())\n",
        "        downloaded_count += 1\n",
        "\n",
        "print(f\"\\nDownload complete! {downloaded_count} new files saved to {download_dir}\")\n",
        "print(f\"Skipped {skipped_count} files that already existed\")\n",
        "\n",
        "# Verify\n",
        "if downloaded_count > 0:\n",
        "    print(\"\\nFirst 5 downloaded files:\")\n",
        "    !ls -lh \"{download_dir}\" | head -5"
      ],
      "metadata": {
        "id": "QRzR5Nu_HXcD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}