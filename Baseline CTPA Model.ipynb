{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# # Install rclone to enable syncing and mounting of OneDrive\n",
        "!curl https://rclone.org/install.sh | sudo bash"
      ],
      "metadata": {
        "id": "3f7592zU1_hR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch rclone configuration interface\n",
        "!rclone config"
      ],
      "metadata": {
        "id": "qHbdZXqe1_BC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update packages and install FUSE3\n",
        "!apt-get update && apt-get install -y fuse3"
      ],
      "metadata": {
        "id": "J499AdQM9jCX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a local directory where OneDrive will be mounted\n",
        "!mkdir -p /content/MyOneDrive"
      ],
      "metadata": {
        "id": "-ropcCKy4bhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the remote OneDrive directory to the local path using rclone\n",
        "!rclone mount MyOneDrive: /content/MyOneDrive --vfs-cache-mode full --allow-other --daemon"
      ],
      "metadata": {
        "id": "sVADBTiI4Ta3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "wUBCNBSO9r5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the CSV files containing the impression text and outcome labels\n",
        "impressions_csv = \"/content/MyOneDrive/GAAP Research Resources/Final_Impressions.csv\"  # Update this path\n",
        "labels_csv = \"/content/MyOneDrive/GAAP Research Resources/Final_Impressions_labels.csv\"              # Update this path\n",
        "\n",
        "# Directory where CT scan files (.nii.gz) are stored\n",
        "download_dir = \"/content/MyOneDrive/GAAP Research Resources/CT + Radiology Impressions Data/CTPA\"                  # Folder with .nii.gz files\n",
        "os.makedirs(download_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "OmLaviiq9r3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSVs into pandas DataFrames\n",
        "impressions_df = pd.read_csv(impressions_csv)\n",
        "labels_df = pd.read_csv(labels_csv)\n",
        "merged_df = pd.merge(impressions_df, labels_df, on=\"impression_id\")\n",
        "\n",
        "# Merge both CSVs on the shared 'impression_id' column\n",
        "merged_df[\"file_path\"] = merged_df[\"impression_id\"].apply(\n",
        "    lambda x: os.path.join(download_dir, f\"{x}.nii.gz\")\n",
        ")\n",
        "\n",
        "# Create a full file path for each .nii.gz file based on impression_id\n",
        "merged_df = merged_df[merged_df[\"file_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(f\"Final usable sample count: {len(merged_df)}\")"
      ],
      "metadata": {
        "id": "n02Y-uGK9r1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into 80/20 split\n",
        "train_df, val_df = train_test_split(merged_df, test_size=0.2, random_state=42)\n",
        "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")"
      ],
      "metadata": {
        "id": "eS2miClGnXUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply windowing (WL: window level, WW: window width) to enhance specific structures in CT slices\n",
        "def window_ct(ct, WL, WW):\n",
        "    ct = np.clip(ct, WL - WW / 2, WL + WW / 2)\n",
        "    return (ct - (WL - WW / 2)) / WW\n",
        "\n",
        "# Load a CT scan and process each 2D slice\n",
        "def process_ct_scan(path):\n",
        "    try:\n",
        "        # Load 3D volume data from the .nii.gz file\n",
        "        ct = nib.load(path).get_fdata()\n",
        "        slices = []\n",
        "        for i in range(ct.shape[-1]):\n",
        "            slice_ = ct[:, :, i] # Get one 2D slice\n",
        "            # Resize to 256x256\n",
        "            slice_resized = T.functional.resize(T.functional.to_pil_image(slice_), [256, 256])\n",
        "            # Crop the center 224x224 region\n",
        "            slice_cropped = T.functional.center_crop(slice_resized, [224, 224])\n",
        "            # Convert to NumPy array and float32 format\n",
        "            slice_np = np.array(slice_cropped).astype(np.float32)\n",
        "            # Apply three different window views\n",
        "            lung     = window_ct(slice_np, WL=-600, WW=1500)\n",
        "            pe       = window_ct(slice_np, WL=100, WW=700)\n",
        "            mediast  = window_ct(slice_np, WL=40, WW=400)\n",
        "            # Stack the three windows\n",
        "            stacked = np.stack([lung, pe, mediast], axis=-1)  # (224, 224, 3)\n",
        "            slices.append(stacked)\n",
        "        # Stack all processed slices into one 4D tensor\n",
        "        return np.stack(slices)  # (N, 224, 224, 3)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping file {path} due to error: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "HKIbIa5K9ryw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom PyTorch Dataset for loading and transforming CT scans\n",
        "class CTScanDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        # Normalization values from ImageNet\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std  = [0.229, 0.224, 0.225]\n",
        "        self.normalize = T.Normalize(mean=self.mean, std=self.std)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        ct_path = row[\"file_path\"]\n",
        "        # Process the CT scan into a stack of slices\n",
        "        slices = process_ct_scan(ct_path)  # (N, 224, 224, 3)\n",
        "\n",
        "        if slices is None:  # Skip if processing failed\n",
        "            return None\n",
        "\n",
        "        # Reorder axes to (num_slices, channels, height, width)\n",
        "        slices = slices.transpose(0, 3, 1, 2)  # (N, 3, 224, 224)\n",
        "\n",
        "        # Convert to tensor and normalize each slice individually\n",
        "        slices_tensor = torch.tensor(slices, dtype=torch.float32)\n",
        "        slices_tensor = torch.stack([self.normalize(slice_) for slice_ in slices_tensor])\n",
        "\n",
        "        # Convert string labels to numerical values\n",
        "        label_values = []\n",
        "        for col in [\"1_month_readmission\", \"6_month_readmission\", \"12_month_readmission\", \"pe_positive\"]:\n",
        "            value = row[col]\n",
        "            if isinstance(value, str):\n",
        "                if value.upper() == 'TRUE':\n",
        "                    label_values.append(1.0)\n",
        "                elif value.upper() == 'FALSE':\n",
        "                    label_values.append(0.0)\n",
        "                else:  # Handle 'Censored' or other unexpected strings\n",
        "                    label_values.append(0.0)\n",
        "            else:\n",
        "                label_values.append(float(value))\n",
        "\n",
        "        label = np.array(label_values).astype(np.float32)\n",
        "\n",
        "        return slices_tensor, torch.tensor(label)"
      ],
      "metadata": {
        "id": "pL5PbdTl9rwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LRCN(nn.Module):\n",
        "    def __init__(self, hidden_size=128, num_layers=1):\n",
        "        super().__init__()\n",
        "        # Use a pretrained ResNet18 as a feature extractor\n",
        "        base_cnn = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.cnn = nn.Sequential(*list(base_cnn.children())[:-1])  # Output shape: (batch, 512, 1, 1)\n",
        "        # LSTM to process sequence of feature vectors\n",
        "        self.rnn = nn.LSTM(input_size=512, hidden_size=hidden_size, batch_first=True, num_layers=num_layers)\n",
        "        # Final classification layer to predict 4 outcomes\n",
        "        self.fc = nn.Linear(hidden_size, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Batch, Time (slices), Channels, Height, Width\n",
        "        B, T, C, H, W = x.shape\n",
        "        # Merge batch and time to process all slices at once\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        # Extract features using CNN\n",
        "        with torch.no_grad():\n",
        "            x = self.cnn(x).squeeze() # Shape: (B*T, 512)\n",
        "        # Reshape back to sequence format\n",
        "        x = x.view(B, T, -1) # Shape: (B, T, 512)\n",
        "        # Pass through LSTM\n",
        "        x, _ = self.rnn(x)\n",
        "        # Use last time step's output for prediction\n",
        "        x = x[:, -1, :]\n",
        "        # Final layer for multi-label binary classification\n",
        "        return torch.sigmoid(self.fc(x))"
      ],
      "metadata": {
        "id": "UoiS7VS29ruC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c7cc484"
      },
      "source": [
        "# Select device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Skips failed data samples\n",
        "def custom_collate_fn(batch):\n",
        "    batch = [item for item in batch if item is not None]\n",
        "    if not batch:\n",
        "        return None, None  # Return None for both data and label if batch is empty\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# Ensures dataset is not empty before training\n",
        "if merged_df.empty:\n",
        "    print(\"Error: No matching files found after filtering. Cannot proceed with training.\")\n",
        "else:\n",
        "    # Create dataset and dataloader\n",
        "    train_dataset = CTScanDataset(train_df)\n",
        "    val_dataset = CTScanDataset(val_df)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    model = LRCN().to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    # Train the model for 5 epochs\n",
        "    model.train()\n",
        "    for epoch in range(5):\n",
        "        total_loss = 0\n",
        "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch + 1} - Training\"):\n",
        "            if x is None or y is None:\n",
        "                continue\n",
        "            # Move data to GPU/CPU\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            # Forward + backward + optimization\n",
        "            optimizer.zero_grad()\n",
        "            out = model(x)\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        # Print average loss per epoch\n",
        "        avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0\n",
        "        print(f\"Epoch {epoch + 1} Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in tqdm(val_loader, desc=f\"Epoch {epoch + 1} - Validation\"):\n",
        "                if x_val is None or y_val is None:\n",
        "                    continue\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "                out_val = model(x_val)\n",
        "                val_loss = criterion(out_val, y_val)\n",
        "                total_val_loss += val_loss.item()\n",
        "        # Print average loss per epoch\n",
        "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
        "        print(f\"Epoch {epoch + 1} Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Set back to train mode for next epoch\n",
        "        model.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}