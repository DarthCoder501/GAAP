{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/NM89LC9oH9ggFI+IsmAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthCoder501/GAAP/blob/main/Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qDsfc5SRgk1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Read the original CSV file\n",
        "df = pd.read_csv('/content/image_ehr_crosswalk_20250418.csv')\n",
        "\n",
        "\n",
        "# 1. Drop rows where note_id == -1\n",
        "df = df[df['note_id'] != -1]\n",
        "\n",
        "\n",
        "# Group by person_id and check for multiple unique image_ids\n",
        "repeating_persons = df.groupby('person_id').filter(lambda x: x['image_id'].nunique() > 1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "i8JgWRCHRmeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Found {len(repeating_persons)} records with repeating person_ids and different image_ids\")"
      ],
      "metadata": {
        "id": "6LEvqBu_RjG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to a new CSV file\n",
        "repeating_persons.to_csv('filtered_repeating_persons.csv', index=False)"
      ],
      "metadata": {
        "id": "QZWWA4ziRjEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read both CSV files\n",
        "df1 = pd.read_csv('/content/Final_Impressions.csv')  # contains 'impression_id'\n",
        "df2 = pd.read_csv('/content/filtered_repeating_persons.csv')  # contains 'image_id' and 'note_id'\n",
        "\n",
        "# First filter df2 to exclude rows with note_id == -1\n",
        "df2_filtered = df2[df2['note_id'] != -1]\n"
      ],
      "metadata": {
        "id": "AqP36VAwRi_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_filtered = df2_filtered.drop(\"impression_id\", axis=1)"
      ],
      "metadata": {
        "id": "C4J5mGsAGGTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then rename image_id to impression_id for merging\n",
        "df2_filtered = df2_filtered.rename(columns={'image_id': 'impression_id'})\n",
        "\n",
        "# Perform an inner merge - this automatically keeps only impression_ids that exist in both\n",
        "merged_df = pd.merge(\n",
        "    df1,\n",
        "    df2_filtered,\n",
        "    on='impression_id',\n",
        "    how='inner'\n",
        ")\n"
      ],
      "metadata": {
        "id": "GNR_V_lCGGRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "kO4bNhS7GGOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"note_id\", axis=1)"
      ],
      "metadata": {
        "id": "X-ANXpQRYim9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "ClgQI2iWY8wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result\n",
        "merged_df.to_csv('filtered_merged_output.csv', index=False)"
      ],
      "metadata": {
        "id": "FX8_Al0GRi9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load both CSVs\n",
        "DF1 = pd.read_csv('/content/filtered_merged_output.csv')  # Keep rows from here\n",
        "df2 = pd.read_csv('/content/Final_Impressions_labels.csv')  # Has the balanced impression_ids"
      ],
      "metadata": {
        "id": "cdlpS7h1Zii7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter df1 to only keep rows where impression_id is in df2\n",
        "filtered_df1 = df1[df1['impression_id'].isin(df2['impression_id'])]"
      ],
      "metadata": {
        "id": "M7nVQd6AZdhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df1"
      ],
      "metadata": {
        "id": "kfr-oSnRZdeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(\n",
        "    DF1,\n",
        "    df2,\n",
        "    on='impression_id',\n",
        "    how='inner'\n",
        ")"
      ],
      "metadata": {
        "id": "m-LGZkuPZdcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "pXUz8kryZdZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"1_month_mortality\", axis=1)"
      ],
      "metadata": {
        "id": "I1CfihScZdW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"6_month_mortality\", axis=1)"
      ],
      "metadata": {
        "id": "5-J8Vcuxa6sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"12_month_mortality\", axis=1)"
      ],
      "metadata": {
        "id": "fodnY6eqa6qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"12_month_PH\", axis=1)"
      ],
      "metadata": {
        "id": "4aznry1ma6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"pe_acute\", axis=1)"
      ],
      "metadata": {
        "id": "P3vjw4Cua6lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = merged_df.drop(\"pe_subsegmentalonly\", axis=1)"
      ],
      "metadata": {
        "id": "7tF35HVza6iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df"
      ],
      "metadata": {
        "id": "5WYX8RAma6fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Value to drop\n",
        "value_to_drop = 'Censored'"
      ],
      "metadata": {
        "id": "BfDy_LKwa6c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the boolean mask\n",
        "mask = (merged_df['1_month_readmission'] != value_to_drop) & \\\n",
        "       (merged_df['6_month_readmission'] != value_to_drop) & \\\n",
        "       (merged_df['12_month_readmission'] != value_to_drop)\n",
        "\n",
        "# Apply the mask to keep rows that don't contain 'Censored'\n",
        "df_filtered = merged_df[mask]"
      ],
      "metadata": {
        "id": "6HH5iD-wZdUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered"
      ],
      "metadata": {
        "id": "N1Db263PZdRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to identify boolean-like columns (True/False or 0/1)\n",
        "def imbalance(series):\n",
        "    return set(series.dropna().unique()).issubset({True, False, 0, 1, 'TRUE', 'FALSE', 'true', 'false'})\n",
        "\n",
        "# Normalize values: convert TRUE/FALSE strings to bool\n",
        "preprocess_df = df_filtered.replace({'TRUE': True, 'FALSE': False, 'true': True, 'false': False})\n",
        "\n",
        "# Identify boolean-like columns (excluding ID)\n",
        "bool_cols = [col for col in preprocess_df.columns if col != 'impression_id' and imbalance(df_filtered[col])]\n",
        "\n",
        "# Count True vs False or 1 vs 0 in each boolean column\n",
        "for col in bool_cols:\n",
        "    counts = preprocess_df[col].value_counts(dropna=False)\n",
        "    print(f\"\\nColumn: {col}\")\n",
        "    print(counts)\n",
        "    print(\"Class Balance (%):\")\n",
        "    print(preprocess_df[col].value_counts(normalize=True, dropna=False) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtEeIQ_wfN2T",
        "outputId": "90342a67-d060-4115-8e5e-f63b9fd26c00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Column: 1_month_readmission\n",
            "1_month_readmission\n",
            "False    5268\n",
            "True      290\n",
            "Name: count, dtype: int64\n",
            "Class Balance (%):\n",
            "1_month_readmission\n",
            "False    94.782296\n",
            "True      5.217704\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Column: 6_month_readmission\n",
            "6_month_readmission\n",
            "False    4762\n",
            "True      796\n",
            "Name: count, dtype: int64\n",
            "Class Balance (%):\n",
            "6_month_readmission\n",
            "False    85.678302\n",
            "True     14.321698\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Column: 12_month_readmission\n",
            "12_month_readmission\n",
            "False    4487\n",
            "True     1071\n",
            "Name: count, dtype: int64\n",
            "Class Balance (%):\n",
            "12_month_readmission\n",
            "False    80.730479\n",
            "True     19.269521\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Column: pe_positive\n",
            "pe_positive\n",
            "0    4240\n",
            "1    1318\n",
            "Name: count, dtype: int64\n",
            "Class Balance (%):\n",
            "pe_positive\n",
            "0    76.286434\n",
            "1    23.713566\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-30-3647954463.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  preprocess_df = df_filtered.replace({'TRUE': True, 'FALSE': False, 'true': True, 'false': False})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.to_csv(\"Progression Dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "HPbMFPi9eBrR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}